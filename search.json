[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: Byeong-Hak Choe\nEmail: bchoe@geneseo.edu\nPhone: (585) 245-5425\nClass Websites:\n\nGitHub Course Website\nBrightspace Course Shell\n\nOffice: South Hall 227B\nOffice Hours:\n\nMondays and Wednesdays 3:30 P.M. – 5:00 P.M.\nBy appointment via email"
  },
  {
    "objectID": "syllabus.html#data-preparation-and-statistical-methods",
    "href": "syllabus.html#data-preparation-and-statistical-methods",
    "title": "Syllabus",
    "section": "Data Preparation and Statistical Methods",
    "text": "Data Preparation and Statistical Methods\n\nModern Business Analytics – Matt Taddy\nAn Introduction to Statistical Learning – James, Witten, Hastie, Tibshirani (Free PDF)\nStatistical Inference via Data Science – Ismay & Kim (Online)\nPractical Data Science with R – Zumel & Mount (Online)\nR for Data Science – Wickham & Grolemund (Free Online)\nFoundations of Data Science with Python – Shea (Online)\nPython Data Science Handbook – VanderPlas (Free Online)\nPython for Data Analysis – McKinney (Free Online)\nCoding for Economists – Turrell (Online)"
  },
  {
    "objectID": "syllabus.html#reproducible-documentation",
    "href": "syllabus.html#reproducible-documentation",
    "title": "Syllabus",
    "section": "Reproducible Documentation",
    "text": "Reproducible Documentation\n\nQuarto Guide"
  },
  {
    "objectID": "syllabus.html#advanced-data-visualization",
    "href": "syllabus.html#advanced-data-visualization",
    "title": "Syllabus",
    "section": "Advanced Data Visualization",
    "text": "Advanced Data Visualization\n\nFundamentals of Data Visualization – Wilke (Free Online)\n\nData Visualization: A Practical Introduction – Healy (Free Online)\n\nR Graphics Cookbook – Chang (Free Online)\n\nggplot2: Elegant Graphics for Data Analysis – Wickham et al. (Free Online)"
  },
  {
    "objectID": "mba/mba-ch1-lm.html",
    "href": "mba/mba-ch1-lm.html",
    "title": "Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr) \nlibrary(ggfortify) # to create regression-related plots\nlibrary(ggcorrplot) # to create correlation heatmaps\nlibrary(fastDummies) # to create dummy variables\nlibrary(stargazer) # to create regression tables\n\noj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')"
  },
  {
    "objectID": "mba/mba-ch1-lm.html#loading-packages-and-data",
    "href": "mba/mba-ch1-lm.html#loading-packages-and-data",
    "title": "Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr) \nlibrary(ggfortify) # to create regression-related plots\nlibrary(ggcorrplot) # to create correlation heatmaps\nlibrary(fastDummies) # to create dummy variables\nlibrary(stargazer) # to create regression tables\n\noj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')"
  },
  {
    "objectID": "mba/mba-ch1-lm.html#exploratory-data-analysis",
    "href": "mba/mba-ch1-lm.html#exploratory-data-analysis",
    "title": "Linear Regression",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\nskim(oj)\n\n\nData summary\n\n\nName\noj\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nbrand\n0\n1\n9\n11\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\n0\n1\n17312.21\n27477.66\n64.00\n4864.00\n8384.00\n17408.00\n716416.00\n▇▁▁▁▁\n\n\nprice\n0\n1\n2.28\n0.65\n0.52\n1.79\n2.17\n2.73\n3.87\n▁▆▇▅▂\n\n\nad\n0\n1\n0.24\n0.43\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\n\n\noj %&gt;% group_by(brand) %&gt;% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nbrand\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nbrand\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\ndominicks\n0\n1\n19834.56\n32245.47\n64.00\n4416.00\n9152.00\n21056.00\n716416.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n0\n1\n18238.46\n29992.21\n320.00\n4800.00\n8320.00\n18560.00\n591360.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n0\n1\n13863.62\n17515.82\n192.00\n5248.00\n8000.00\n13824.00\n288384.00\n▇▁▁▁▁\n\n\nprice\ndominicks\n0\n1\n1.74\n0.39\n0.52\n1.58\n1.59\n1.99\n2.69\n▁▂▇▃▂\n\n\nprice\nminute.maid\n0\n1\n2.24\n0.40\n0.88\n1.99\n2.17\n2.49\n3.17\n▁▂▇▆▂\n\n\nprice\ntropicana\n0\n1\n2.87\n0.55\n1.29\n2.49\n2.99\n3.19\n3.87\n▁▃▅▇▅\n\n\nad\ndominicks\n0\n1\n0.26\n0.44\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nad\nminute.maid\n0\n1\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nad\ntropicana\n0\n1\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\n\n\noj %&gt;% group_by(brand, ad) %&gt;% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nbrand, ad\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nbrand\nad\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\ndominicks\n0\n0\n1\n11733.91\n15718.77\n64.00\n3584.00\n7040.00\n13312.00\n248000.00\n▇▁▁▁▁\n\n\nsales\ndominicks\n1\n0\n1\n43251.33\n50930.45\n64.00\n11888.00\n27744.00\n54160.00\n716416.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n0\n0\n1\n8372.65\n7701.69\n320.00\n4160.00\n6272.00\n9984.00\n231808.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n1\n0\n1\n42566.32\n46260.27\n1728.00\n16576.00\n29440.00\n49808.00\n591360.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n0\n0\n1\n10038.86\n10246.02\n192.00\n4864.00\n7168.00\n11008.00\n175872.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n1\n0\n1\n33047.02\n29632.94\n1408.00\n10816.00\n23808.00\n46896.00\n288384.00\n▇▂▁▁▁\n\n\nprice\ndominicks\n0\n0\n1\n1.80\n0.38\n0.52\n1.58\n1.69\n1.99\n2.69\n▁▂▇▅▂\n\n\nprice\ndominicks\n1\n0\n1\n1.56\n0.34\n0.89\n1.39\n1.58\n1.59\n2.69\n▂▇▂▁▁\n\n\nprice\nminute.maid\n0\n0\n1\n2.33\n0.41\n0.88\n1.99\n2.26\n2.62\n3.17\n▁▂▇▇▃\n\n\nprice\nminute.maid\n1\n0\n1\n2.02\n0.30\n0.99\n1.99\n1.99\n2.19\n2.81\n▁▂▇▂▂\n\n\nprice\ntropicana\n0\n0\n1\n2.97\n0.51\n1.32\n2.59\n2.99\n3.39\n3.87\n▁▂▃▇▅\n\n\nprice\ntropicana\n1\n0\n1\n2.39\n0.46\n1.29\n1.99\n2.39\n2.79\n3.59\n▁▇▆▅▂\n\n\n\n\n\n\n\n\nData Visualization\n\nCorrelation heatmap is a great tool to start identifying which input variables are strongly correlated with an outcome variable.\n\n\n# to convert a factor variable into indicators\noj_dummies &lt;- dummy_cols(oj, select_columns = 'brand' ) %&gt;% \n  select(-brand)\n\n# the matrix of the correlation test p-values\np.mat &lt;- cor_pmat(oj_dummies) \n\n# correlation heatmap with correlation values\nggcorrplot( cor(oj_dummies), lab = T,\n            type = 'lower',\n            colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n            p.mat = p.mat) # p.values\n\n\n\n# variation in log price\nggplot(oj, aes(x = log(price), fill = brand )) +\n  geom_histogram() +\n  facet_wrap(brand ~., ncol = 1)\n\n\n\n# variation in log sales\nggplot(oj, aes(x = log(sales), fill = brand )) +\n  geom_histogram() +\n  facet_wrap(brand ~., ncol = 1)\n\n\n\n# law of demand\np &lt;- ggplot(oj, aes(x = log(sales), y = log(price),\n                    color = brand ))\n\np + geom_point( alpha = .025 ) +\n  geom_smooth(method = lm, se = F)\n\n\n\n# mosaic plot\nggplot(data = oj) +\n  geom_bar(aes(x = as.factor(ad), y = after_stat(prop),\n               group = brand, fill = brand), \n           position = \"fill\") +\n  labs(x = 'ad') +\n  theme(plot.title = element_text(size = rel(1.5)),\n        axis.title = element_text(size = 25),\n        axis.text.x = element_text(size = rel(1.5)),\n        axis.text.y = element_text(size = rel(1.5)))"
  },
  {
    "objectID": "mba/mba-ch1-lm.html#linear-regression-model",
    "href": "mba/mba-ch1-lm.html#linear-regression-model",
    "title": "Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nA basic but powerful regression strategy is to deal in averages and lines.\n\nWe model the conditional mean for \\(y\\) given \\(x\\) as\n\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}\\\\\n{ }\\\\\ny_{i} &=   \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1, i} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p, i} + \\epsilon_{i} \\quad \\text{for } i = 1, 2, ..., n\n\\end{align}\n\\] - Linear regression is used to model linear relationship between an outcome variable, \\(y\\), and a set of predictor variables \\(x_{1}, \\,\\cdots\\,, x_{p}\\).\n\nknitr::include_graphics('https://bcdanl.github.io/lec_figs/mba-1-2.png')\n\n\n\n\n\n\n\n\n\n\\(\\beta_{0}\\) is an intercept when \\(\\mathbf{X} = \\mathbf{0}\\).\n\\(\\beta_{1}\\) is a slope that describes a change in average value for \\(y\\) for each one-unit increase in \\(x_{1}\\).\n\\(\\epsilon_{i}\\) is the random noise.\n\nFor inference, we need to assume that \\(\\epsilon_{i}\\) is independent, identically distributed (iid) from Normal distribution.\n\n\n\\[\n\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2) \\quad \\text{ with constant variance } \\sigma^2\n\\]\n\nknitr::include_graphics('https://bcdanl.github.io/lec_figs/mba-1-3.png')\n\n\n\n\n\n\n\n\n\n\nFitted Line and Beta Estimates\n\nWe estimatede the best fitting line by ordinary least squares (OLS) - by minimizing the sum of squared errors (SSE)\n\n\\[\nS S E\\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n}\\left[Y_{i}-\\left({\\beta}_{0}+{\\beta}_{1} X_{i}\\right)\\right]^{2}\n\\]\n\nknitr::include_graphics('https://bcdanl.github.io/lec_figs/mba-1-9.png')\n\n\n\n\n\n\n\n\nTherefore, the beta estimate has the following solution:\n\\[\n\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}} \\quad \\text{ and } \\quad \\widehat{\\beta}_{0}=\\bar{Y}-\\widehat{\\beta}_{1} \\bar{X}\n\\]\nwhere \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) and \\(\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\)\n\n\n\nConnection to covariance and correlation\n\nCovariance describes the joint variability of two variables\n\n\\[\n\\text{Cov}(X, Y) = \\sigma_{X,Y} = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n\\]\n\nCorrelation is a normalized form of covariance, ranges from -1 to 1\n\n\\[\n\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\] - So, the beta coefficent can be represented by:\n$$ \\[\\begin{align}\n\\widehat{\\beta}_{1} &= \\, \\frac{\\widehat{\\text{Cov}}(X,Y)}{\\widehat{\\text{Var}}(X)} \\,=\\,  \\hat{\\rho}_{x,y} \\cdot \\frac{\\hat{\\sigma}_{Y}}{\\hat{\\sigma}_{X}}\n\n\\end{align}\\] $$\n\n\n\nInference with OLS\n\n\\(t\\)-statistics are coefficients Estimates / Std. Error, i.e., number of standard deviations from 0\n\np-values (i.e., Pr(&gt;|t|)): estimated probability observing value as extreme as |t value| given the null hypothesis \\(\\beta = 0\\)\np-value \\(&lt;\\) conventional threshold of \\(\\alpha = 0.05\\), sufficient evidence to reject the null hypothesis that the coefficient is zero,\nTypically |t values| \\(&gt; 2\\) indicate significant relationship at \\(\\alpha = 0.05\\)\ni.e., there is a significant association between log(sales) and log(price)\n\nControlling the Type 1 error rate at \\(\\alpha = 0.05\\), i.e., the probability of a false positive mistake:\n\n5% chance that you’ll conclude there’s a significant association between \\(x\\) and \\(y\\) even when there is none\n\n\n\n\n\nR-squared\n\n\\(R^2\\) estimates the proportion of the variance of \\(Y\\) explained by \\(X\\).\n\n\n\n\nMean Squared Errors\n\nMean squared error (MSE) is a commonly used metric to evaluate the performance of a regression model.\n\nIt measures the average squared difference between the predicted values and the actual values in the dataset.\n\n\n\\[\nM S E \\,=\\, \\frac{\\sum_{i = 1}^{ n}\\,(\\, y_{i} - \\hat{y}_{i} \\,)^2}{n}\n\\] - Root mean squared error (RMSE) is the square root of the mean squared error (MSE).\n\\[\nR M S E \\,=\\, \\sqrt{M S E}\n\\] - RMSE shows how far predictions fall from true values.\n\n\n\nThe Goals of Linear Regression\n\nModeling for prediction (\\(\\hat{y}\\)): When we want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(\\mathbf{X}\\).\n\n\nWe are estimating the conditional expection (mean) for \\(y\\): \\[\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] = \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}.\n\\]\nwhich is the average value for \\(y\\) given the value for \\(X\\).\n\n\n\nModeling for explanation (\\(\\hat{\\beta}\\)): When we want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(\\mathbf{X}\\).\n\n\nCorrelation does not imply causation.\nWithout proper identification strategies, \\(\\beta_{1}\\) just means a correlation between \\(x_{1}\\) and \\(y\\).\nHowever, we can possibly identify a causal relationship between the explanatory variable and the outcome variable."
  },
  {
    "objectID": "mba/mba-ch1-lm.html#linear-regression-and-controls",
    "href": "mba/mba-ch1-lm.html#linear-regression-and-controls",
    "title": "Linear Regression",
    "section": "Linear Regression and Controls",
    "text": "Linear Regression and Controls\n\nSimple Linear Regression\nTo start, we can fit a simple model that regresses log price on log sales.\n\\[\n\\mathbb{E}[\\, \\log(\\, \\texttt{sales} \\,) \\,|\\, \\texttt{price}\\,] = \\alpha \\,+\\, \\beta\\, \\log(\\,\\texttt{price}\\,)\n\\] - The following model incorporates both brand and price:\n\\[\n\\mathbb{E}[\\, \\log(\\, \\texttt{sales} \\,) \\,|\\, \\texttt{price}\\,] = \\alpha_{\\texttt{brand}} \\,+\\, \\beta\\, \\log(\\,\\texttt{price}\\,)\n\\]\nformula_0 &lt;- log(sales) ~ log(price)\nformula_1 &lt;- log(sales) ~ brand + log(price)\n\nfit_0 &lt;- lm( formula_0, data = oj )\nfit_1 &lt;- lm( formula_1, data = oj )\n\nstargazer(fit_0, fit_1, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(sales)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nbrandminute.maid\n\n\n\n\n0.870***\n\n\n\n\n\n\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana\n\n\n\n\n1.530***\n\n\n\n\n\n\n\n\n(0.016)\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price)\n\n\n-1.601***\n\n\n-3.139***\n\n\n\n\n\n\n(0.018)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n10.423***\n\n\n10.829***\n\n\n\n\n\n\n(0.015)\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28,947\n\n\n28,947\n\n\n\n\nR2\n\n\n0.208\n\n\n0.394\n\n\n\n\nAdjusted R2\n\n\n0.208\n\n\n0.394\n\n\n\n\nResidual Std. Error\n\n\n0.907 (df = 28945)\n\n\n0.794 (df = 28943)\n\n\n\n\nF Statistic\n\n\n7,608.212*** (df = 1; 28945)\n\n\n6,275.074*** (df = 3; 28943)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nWe know that there are different brands of OJ here and some are more valuable than others.\n\nWhen we control for brand effect, the elasticity estimate nearly doubles to −3.14.\n\nThe premium brands, Minute Maid and Tropicana, had equivalent sales to Dominick’s at higher price points.\nSo if we don’t control for brand, it looks as though prices can rise without affecting sales for those observations.\n\nThis dampens the observable relationship between prices and sales and results in the (artificially) low elasticity estimate of −1.6.\n\nMore mechanically, how does this happen in regression?\n\n\nprice_fit &lt;- lm(log(price) ~ brand, data = oj)\np_hat &lt;- predict(price_fit, newdata = oj)\np_resid &lt;- log(oj$price) - p_hat\n\n# regress log sales on p_resid\nresid_fit &lt;- lm(log(sales) ~ p_resid, data = oj)\n\n# What is the beta coefficient for p_resid?!\nround( coef(resid_fit)[2], digit = 3 )\n\np_resid \n -3.139 \n\n\n\nThe coefficient on p_resid, the residuals from regression of log price on brand, is exactly the same as what we get on log(price) in the multiple linear regression for log sales onto this and brand!\nThis is one way that you can understand what OLS is doing:\n\nIt is finding the coefficients on the part of each input that is independent from the other inputs.\n\n\n\n\n\nControls in Linear Regression\n\nOmitted variable bias is a type of bias that can occur in linear regression when an important variable that is related to both the outcome variable and the input variable(s) is not included in the model.\n\nThis omission can lead to biased estimates of the beta coefficients of the included input variables.\n\nIn linear regression, a confounding variable is a variable that is related to both treatment and outcome variables, and that affects the relationship between them.\n\nA treatment variable is an input variable that the researcher believes has a causal effect on the outcome variable.\nWhen a confounding variable is not controlled in the regression model, it can lead to biased estimates of the relationship between the independent and treatment variables.\n\nBad controls in linear regression refer to the inclusion of variables in the model that do not actually control for the confounding factors they are intended to control for.\n\nThis can lead to biased estimates of the relationship between the independent and dependent variables.\n\n\n# simulation data\ntb &lt;- tibble( \n  female = ifelse(runif(10000)&gt;=0.5,1,0), # female indicator variable\n  ability = rnorm(10000), # e.g., talent, usually unobserved.\n  discrimination = female, # gender discrimination variable\n  occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10000), # true data generating process for occupation variable\n  wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10000) # true data generating process for wage variable\n)\n\nlm_1 &lt;- lm(wage ~ female, tb)\nlm_2 &lt;- lm(wage ~ female + occupation, tb)\nlm_3 &lt;- lm(wage ~ female + occupation + ability, tb)\n\nstargazer(lm_1,lm_2,lm_3, \n          column.labels = c(\"Biased Unconditional\", \n                            \"Biased\",\n                            \"Unbiased Conditional\"),\n          type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nwage\n\n\n\n\n\n\nBiased Unconditional\n\n\nBiased\n\n\nUnbiased Conditional\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nfemale\n\n\n-3.012***\n\n\n0.622***\n\n\n-0.991***\n\n\n\n\n\n\n(0.085)\n\n\n(0.030)\n\n\n(0.028)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noccupation\n\n\n\n\n1.804***\n\n\n0.993***\n\n\n\n\n\n\n\n\n(0.006)\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nability\n\n\n\n\n\n\n2.026***\n\n\n\n\n\n\n\n\n\n\n(0.022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n1.952***\n\n\n0.207***\n\n\n1.005***\n\n\n\n\n\n\n(0.060)\n\n\n(0.020)\n\n\n(0.017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n10,000\n\n\n10,000\n\n\n10,000\n\n\n\n\nR2\n\n\n0.112\n\n\n0.909\n\n\n0.950\n\n\n\n\nAdjusted R2\n\n\n0.112\n\n\n0.909\n\n\n0.950\n\n\n\n\nResidual Std. Error\n\n\n4.237 (df = 9998)\n\n\n1.355 (df = 9997)\n\n\n1.004 (df = 9996)\n\n\n\n\nF Statistic\n\n\n1,263.492*** (df = 1; 9998)\n\n\n50,026.950*** (df = 2; 9997)\n\n\n63,578.110*** (df = 3; 9996)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\noccupation is a bad control.\n\n\n\n\nResidual plot, QQ plot, and Residual vs Leverage plot\n\nResiduals should NOT display any systematic pattern.\nCheck the assumptions about normality with a QQ plot using ggfortify.\n\n\nlibrary(ggfortify)\nautoplot(fit_1, ncol = 1)\n\n\n\n\n\nStandardized residuals = residuals / sd(residuals)\nA QQ (Quantile-Quantile) plot is a graphical tool used to assess the normality of a distribution.\n\nIn the context of linear regression, a QQ plot can be used to assess whether the residuals are normally distributed.\nA QQ plot visualizes the relationship between the quantiles of the residuals and the quantiles of a theoretical normal distribution.\nQQ plots can be useful in identifying potential outliers or influential observations in a linear regression model, as well as in deciding whether to transform the dependent variable or use a different type of regression model altogether.\n\nA residual vs leverage plot is a graphical tool used to detect influential observations in linear regression.\n\nLeverage refers to how much an observation’s independent variables differ from the mean of the independent variables, and is a measure of how much influence that observation has on the regression line.\nIn a residual vs leverage plot, influential observations will typically appear as points that are far away from the center of the plot.\nIf an observation has high leverage but a small residual, it may not be influential.\nConversely, an observation with a large residual but low leverage may also not be influential."
  },
  {
    "objectID": "mba/mba-ch1-lm.html#fixed-effects",
    "href": "mba/mba-ch1-lm.html#fixed-effects",
    "title": "Linear Regression",
    "section": "Fixed effects",
    "text": "Fixed effects\n\nUnobserved characteristics that is associated with at least one explanatory variable is called a “fixed effect”.\nIf there exists the fixed effect in the data and we do not control the fixed effect in the model, the model will have a biased result in estimation.\nWe often control unobserved characteristics across countries/individuals/firms/counties/US states by including dummy variables of those entities in the model.\n\nFor example, some unobserved characteristics across countries in the world (like governance effectiveness, investment in electricity grid, etc.) is likely to be associated with variable of access to electricity.\nIf such unobserved characteristics are not controlled in the model, the model result will be likely to be biased.\nThis is because one of the most crucial assumptions in regression—errors are not correlated with explanatory variables \\(E[\\,\\epsilon_{i} | X\\,] \\,=\\, 0\\)—is not likely to hold without controlling such unobserved characteristics.\nThe following is the OLS estimator \\(\\hat{\\beta}\\) for the true beta \\(\\beta\\) in a matrix form.\n\n\n\\[\\begin{align} \\hat{\\beta} \\,=\\,& \\beta + E[\\,(X'X)^{-1}X'\\epsilon\\,]\\\\\n\\,\\neq\\,& \\beta \\qquad \\text{ if } E[\\,\\epsilon_{i} | X\\,] \\,\\neq\\, 0\\end{align}\\]\n- Without controlling such unobserved characteristics, the beta estimator cannot be the same as the true beta at all."
  },
  {
    "objectID": "mba/mba-ch1-lm.html#exercise",
    "href": "mba/mba-ch1-lm.html#exercise",
    "title": "Linear Regression",
    "section": "Exercise",
    "text": "Exercise\nConsider the orange juice models:\n\nformula_0 &lt;- log(sales) ~ log(price)\nformula_1 &lt;- log(sales) ~ brand + log(price)\n\nfit_0 &lt;- lm( formula_0, data = oj )\nfit_1 &lt;- lm( formula_1, data = oj )\n\n\nDraw a residual plot for each model of fit_0 and fit_1.\nCalculate the RMSE for each model of fit_0 and fit_1.\nReview the interaction models:\n\nformula_0 &lt;- log(sales) ~ log(price)\nformula_1 &lt;- log(sales) ~ brand + log(price)\nformula_2 &lt;- log(sales) ~ brand * log(price)\nformula_3 &lt;- log(sales) ~ brand * ad * log(price)\n\nfit_0 &lt;- lm( formula_0, data = oj )\nfit_1 &lt;- lm( formula_1, data = oj )\nfit_2 &lt;- lm( formula_2, data = oj )\nfit_3 &lt;- lm( formula_3, data = oj )\n\nstargazer(fit_1, fit_2, fit_3, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(sales)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nbrandminute.maid\n\n\n0.870***\n\n\n0.888***\n\n\n0.047\n\n\n\n\n\n\n(0.013)\n\n\n(0.042)\n\n\n(0.047)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana\n\n\n1.530***\n\n\n0.962***\n\n\n0.708***\n\n\n\n\n\n\n(0.016)\n\n\n(0.046)\n\n\n(0.051)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nad\n\n\n\n\n\n\n1.094***\n\n\n\n\n\n\n\n\n\n\n(0.038)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price)\n\n\n-3.139***\n\n\n-3.378***\n\n\n-2.774***\n\n\n\n\n\n\n(0.023)\n\n\n(0.036)\n\n\n(0.039)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:ad\n\n\n\n\n\n\n1.173***\n\n\n\n\n\n\n\n\n\n\n(0.082)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:ad\n\n\n\n\n\n\n0.785***\n\n\n\n\n\n\n\n\n\n\n(0.099)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:log(price)\n\n\n\n\n0.057\n\n\n0.783***\n\n\n\n\n\n\n\n\n(0.057)\n\n\n(0.061)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:log(price)\n\n\n\n\n0.666***\n\n\n0.736***\n\n\n\n\n\n\n\n\n(0.054)\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nad:log(price)\n\n\n\n\n\n\n-0.471***\n\n\n\n\n\n\n\n\n\n\n(0.074)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:ad:log(price)\n\n\n\n\n\n\n-1.109***\n\n\n\n\n\n\n\n\n\n\n(0.122)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:ad:log(price)\n\n\n\n\n\n\n-0.986***\n\n\n\n\n\n\n\n\n\n\n(0.124)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n10.829***\n\n\n10.955***\n\n\n10.407***\n\n\n\n\n\n\n(0.015)\n\n\n(0.021)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28,947\n\n\n28,947\n\n\n28,947\n\n\n\n\nR2\n\n\n0.394\n\n\n0.398\n\n\n0.535\n\n\n\n\nAdjusted R2\n\n\n0.394\n\n\n0.398\n\n\n0.535\n\n\n\n\nResidual Std. Error\n\n\n0.794 (df = 28943)\n\n\n0.791 (df = 28941)\n\n\n0.695 (df = 28935)\n\n\n\n\nF Statistic\n\n\n6,275.074*** (df = 3; 28943)\n\n\n3,823.404*** (df = 5; 28941)\n\n\n3,031.232*** (df = 11; 28935)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "mba/mba-ch1-lm.html#references",
    "href": "mba/mba-ch1-lm.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\n\nCausal Inference: The Mixtape by Scott Cunningham.\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim.\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\n\n\n\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "mba/mba-ch1-logit.html#logistic-regression-model",
    "href": "mba/mba-ch1-logit.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nIn a regression model, we model the conditional mean for \\(y\\) given \\(x\\) as\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}\\\\\n{ }\\\\\ny_{i} &=   \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1, i} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p, i} + \\epsilon_{i} \\quad \\text{for } i = 1, 2, ..., n\n\\end{align}\n\\] - Logistic regression is used to model a binary response:\n\n\\(y\\) is either 1 or 0 (e.g., True or False).\n\\(\\epsilon_{i}\\) is the random noise from logistic distribution whose cumulative density function is as follows:\n\n\nknitr::include_graphics('https://bcdanl.github.io/lec_figs/mba-1-11.png')\n\n\n\n\n\n\n\n\nwhere \\(z\\) is a linear combination of explanatory variables \\(\\mathbf{X}\\).\n\\[\nf(z) = \\frac{e^{z}}{1 + e^{z}}\n\\] - Since \\(y\\) is either 0 or 1, the conditional mean value of \\(y\\) is the probability:\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\text{Pr}(y = 1 | \\mathbf{X}) \\times 1 \\,+\\,\\text{Pr}(y = 0 | \\mathbf{X}) \\times 0\\\\ &= \\text{Pr}(y = 1 | \\mathbf{X}) \\\\\n{ }\\\\\n\\text{Pr}(y = 1 | \\mathbf{X}) &= f(\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p})\\\\\n&= \\frac{e^{\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}}}{1 + e^{\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}}} \\\\\n\\end{align}\n\\]\n\nThe logistic regression finds the beta coefficients, \\(b_{0}, b_{1}, \\cdots\\), such that the logistic function ranging from 0 to 1\n\n\\[\n\\begin{align}\nf( b_{0} + b_{1}*x_{i,1} + b_{2}*x_{i,2} + \\cdots )\\notag\n\\end{align}\n\\]\nis the best possible estimate of the binary outcome \\(y_{i}\\).\n\n\nInterpretation of Beta Estimates\n\nIn logistic regression, the effect of \\(x_{1}\\) on \\(Pr(y_{i} = 1 | \\mathbf{X})\\) is different across observations \\(i = 1, 2, \\cdots\\):\n\n\nknitr::include_graphics('https://bcdanl.github.io/lec_figs/effect-linear-logit.png')\n\n\n\n\n\n\n\n\n\n\n\nThe Goals of Logistic Regression:\n\nModeling for prediction (\\(\\text{Pr}({y} | \\mathbf{X})\\)): When we want to predict an outcome variable \\(y = 0 ,1\\) based on the information contained in a set of predictor variables \\(\\mathbf{X}\\).\n\n\nWe are estimating the conditional expectation (mean) for \\(y\\): \\[\n\\text{Pr}(\\, y \\,|\\, \\mathbf{X} \\,) = f(\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}).\n\\]\nwhich is the probability that \\(y = 1\\) given the value for \\(X\\) from the logistic function.\nPrediction from the logistic regression with a threshold on the probabilities can be used as a classifier.\n\nIf the probability that the newborn baby i is at risk is greater than the threshold \\(\\theta\\in (0, 1)\\) (\\(\\text{Pr}(y_{i} = 1 | \\mathbf{X}) &gt; \\theta\\)), the baby i is classified as at-risk.\n\nWe can discuss the performance of classifiers later.\n\nAccuracy: When the classifier says this newborn baby is at risk or is not at risk, what is the probability that the model is correct?\nPrecision: If the classifier says this newborn baby is at risk, what’s the probability that the baby is really at risk?\nRecall: Of all the babies at risk, what fraction did the classifier detect?\nThere is a trade-off between recall and precision.\n\n\n\n\nModeling for explanation (\\(\\hat{\\beta}\\)): When we want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(\\mathbf{X}\\).\n\n\nWe can average the marginal effects across the training data (average marginal effect, or AME).\nWe can obtain the marginal effect at an average observation or representative observations in the training data (marginal effect at the mean or at representative values).\nWe can also consider:\nThe AME for subgroup of the data\nThe AME at the mean value of VARIABLE.\n\n\n\n\nExample: Newbron Babies at risk\n\nWe’ll use a sample dataset from the 2010 CDC natality public-use data.\n\nThe data set records information about all US births, including risk factors about the mother and father, and about the delivery.\nNewborn babies are assessed at one and five minutes after birth to determine if a baby needs immediate emergency care or extra medical attention.\n\n\nTask 1. Identify the effects of several risk factors on the probability of atRisk == TRUE. Task 2. Classify ahead of time babies with a higher probability of atRisk == TRUE.\n\n\nLoad Packages and Data\n\n# install.packages(\"margins\")\nlibrary(tidyverse)\nlibrary(margins) # for AME\nlibrary(hrbrthemes) # for ggplot theme, theme_ipsum()\nlibrary(stargazer)\n\ntheme_set(theme_ipsum()) # setting theme_ipsum() default\nload(url(\"https://bcdanl.github.io/data/NatalRiskData.rData\"))\n\n# 50:50 split between training and testing data\ntrain &lt;- filter(sdata,\n                ORIGRANDGROUP &lt;= 5)\ntest &lt;- filter(sdata,\n                ORIGRANDGROUP &gt; 5)\nskim(train)\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n14212\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nlogical\n9\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGESTREC3\n0\n1\nFALSE\n2\n&gt;= : 12651, &lt; 3: 1561\n\n\nDPLURAL\n0\n1\nFALSE\n3\nsin: 13761, twi: 424, tri: 27\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nCIG_REC\n0\n1\n0.09\nFAL: 12913, TRU: 1299\n\n\nULD_MECO\n0\n1\n0.05\nFAL: 13542, TRU: 670\n\n\nULD_PRECIP\n0\n1\n0.03\nFAL: 13854, TRU: 358\n\n\nULD_BREECH\n0\n1\n0.06\nFAL: 13316, TRU: 896\n\n\nURF_DIAB\n0\n1\n0.05\nFAL: 13450, TRU: 762\n\n\nURF_CHYPER\n0\n1\n0.01\nFAL: 14046, TRU: 166\n\n\nURF_PHYPER\n0\n1\n0.04\nFAL: 13595, TRU: 617\n\n\nURF_ECLAM\n0\n1\n0.00\nFAL: 14181, TRU: 31\n\n\natRisk\n0\n1\n0.02\nFAL: 13939, TRU: 273\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPWGT\n0\n1\n153.28\n38.87\n74\n125\n145\n172\n375\n▆▇▂▁▁\n\n\nUPREVIS\n0\n1\n11.17\n4.02\n0\n9\n11\n13\n49\n▃▇▁▁▁\n\n\nDBWT\n0\n1\n3276.02\n582.91\n265\n2985\n3317\n3632\n6165\n▁▁▇▂▁\n\n\nORIGRANDGROUP\n0\n1\n2.53\n1.70\n0\n1\n3\n4\n5\n▇▅▅▅▅\n\n\n\n\n\n\n\n\nLinear Probability Model (LPM)\n\n# linear probability model\nlpm &lt;- lm(atRisk ~ CIG_REC + GESTREC3 + DPLURAL + \n               ULD_MECO + ULD_PRECIP + ULD_BREECH + \n               URF_DIAB + URF_CHYPER + URF_PHYPER + URF_ECLAM, \n             data = train)\n\n\nLPM often works well when it comes to identifying AME.\nCaveats\n\nProbability of \\(y = 1\\) can be beyond [0, 1].\nThe error can’t be distributed Normal with \\(y_{i} \\in \\{0 , 1\\}\\).\nWhen using LPM, we should make standard errors of beta estimates robust to heteroskedasticity—variances of errors are non-constant across observations.\n\n\n\n\n\nLogistic Regression via glm( family = binomial(link = \"logit\") )\nmodel &lt;- glm(atRisk ~ PWGT + UPREVIS + CIG_REC + GESTREC3 + DPLURAL + \n               ULD_MECO + ULD_PRECIP + ULD_BREECH + \n               URF_DIAB + URF_CHYPER + URF_PHYPER + URF_ECLAM, \n             data = train, \n             family = binomial(link = \"logit\") )\nstargazer(model, type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\natRisk\n\n\n\n\n\n\n\n\nPWGT\n\n\n0.004**\n\n\n\n\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\nUPREVIS\n\n\n-0.063***\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\nCIG_REC\n\n\n0.313*\n\n\n\n\n\n\n(0.187)\n\n\n\n\n\n\n\n\n\n\nGESTREC3&lt; 37 weeks\n\n\n1.545***\n\n\n\n\n\n\n(0.141)\n\n\n\n\n\n\n\n\n\n\nDPLURALtriplet or higher\n\n\n1.394***\n\n\n\n\n\n\n(0.499)\n\n\n\n\n\n\n\n\n\n\nDPLURALtwin\n\n\n0.312\n\n\n\n\n\n\n(0.241)\n\n\n\n\n\n\n\n\n\n\nULD_MECO\n\n\n0.818***\n\n\n\n\n\n\n(0.236)\n\n\n\n\n\n\n\n\n\n\nULD_PRECIP\n\n\n0.192\n\n\n\n\n\n\n(0.358)\n\n\n\n\n\n\n\n\n\n\nULD_BREECH\n\n\n0.749***\n\n\n\n\n\n\n(0.178)\n\n\n\n\n\n\n\n\n\n\nURF_DIAB\n\n\n-0.346\n\n\n\n\n\n\n(0.288)\n\n\n\n\n\n\n\n\n\n\nURF_CHYPER\n\n\n0.560\n\n\n\n\n\n\n(0.390)\n\n\n\n\n\n\n\n\n\n\nURF_PHYPER\n\n\n0.162\n\n\n\n\n\n\n(0.250)\n\n\n\n\n\n\n\n\n\n\nURF_ECLAM\n\n\n0.498\n\n\n\n\n\n\n(0.777)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-4.412***\n\n\n\n\n\n\n(0.289)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n14,212\n\n\n\n\nLog Likelihood\n\n\n-1,231.496\n\n\n\n\nAkaike Inf. Crit.\n\n\n2,490.992\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nsummary(model)\n\n\nCall:\nglm(formula = atRisk ~ PWGT + UPREVIS + CIG_REC + GESTREC3 + \n    DPLURAL + ULD_MECO + ULD_PRECIP + ULD_BREECH + URF_DIAB + \n    URF_CHYPER + URF_PHYPER + URF_ECLAM, family = binomial(link = \"logit\"), \n    data = train)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -4.412189   0.289352 -15.249  &lt; 2e-16 ***\nPWGT                      0.003762   0.001487   2.530 0.011417 *  \nUPREVIS                  -0.063289   0.015252  -4.150 3.33e-05 ***\nCIG_RECTRUE               0.313169   0.187230   1.673 0.094398 .  \nGESTREC3&lt; 37 weeks        1.545183   0.140795  10.975  &lt; 2e-16 ***\nDPLURALtriplet or higher  1.394193   0.498866   2.795 0.005194 ** \nDPLURALtwin               0.312319   0.241088   1.295 0.195163    \nULD_MECOTRUE              0.818426   0.235798   3.471 0.000519 ***\nULD_PRECIPTRUE            0.191720   0.357680   0.536 0.591951    \nULD_BREECHTRUE            0.749237   0.178129   4.206 2.60e-05 ***\nURF_DIABTRUE             -0.346467   0.287514  -1.205 0.228187    \nURF_CHYPERTRUE            0.560025   0.389678   1.437 0.150676    \nURF_PHYPERTRUE            0.161599   0.250003   0.646 0.518029    \nURF_ECLAMTRUE             0.498064   0.776948   0.641 0.521489    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2698.7  on 14211  degrees of freedom\nResidual deviance: 2463.0  on 14198  degrees of freedom\nAIC: 2491\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nLogistic regression finds the beta parameters that maximize the log likelihood of the data, given the model, which is equivalent to minimizing the sum of the residual deviance.\n\nWe want to make the likelihood as big as possible.\nWe want to make the deviance as small as possible.\n\n\n\n\n\nLikelihood Function\n\nLikelihood is the probability of our data given the model.\n\n\nknitr::include_graphics('https://bcdanl.github.io/lec_figs/logistic_likelihood.png')\n\n\n\n\n\n\n\n\n\nThe probability that the seven data points would be observed: \\(L = (1-P1)*(1-P2)* P3*(1-P4)*P5*P6*P7\\).\n\nThe log of the likelihood: \\(\\log(L) = \\log(1-P1) + \\log(1-P2) + \\log(P3) + \\log(1-P4) + \\log(P5) + \\log(P6) + \\log(P7)\\)\n\nIn statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data.\n\nThis is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable.\n\n\n\n\n\nDeviance\n\nDeviance measures to the distance between data and fit\nThe null deviance is similar to the variance of the data around the average rate of positive examples.\n\n\n# likelihood function for logistic regression\n## y: the outcome in numeric form, either 1 or 0\n## py: the predicted probability that y == 1\n\nloglikelihood &lt;- function(y, py) {\n  sum( y * log(py) + (1-y)*log(1 - py) )\n}\n\n# the rate of positive example in the dataset\npnull &lt;- mean( as.numeric(train$atRisk) )\n\n# the null deviance\nnull.dev &lt;- -2 *loglikelihood(as.numeric(train$atRisk), pnull)\n\n# the null deviance from summary(model)\nmodel$null.deviance\n\n[1] 2698.716\n\n# the predicted probability for the training data\npred &lt;- predict(model, newdata=train, type = \"response\")\n\n# the residual deviance\nresid.dev &lt;- -2 * loglikelihood(as.numeric(train$atRisk), pred)\n\n# the residual deviance from summary(model)\nmodel$deviance\n\n[1] 2462.992\n\n\n\n\n\nAIC, AICc, BIC and pseudo R-squared\n\nThe AIC, or the Akaike information criterion, is the log likelihood adjusted for the number of coefficients.\n\nThe corrected AIC, AICc, is the AIC corrected by the sample size and the degree of freedom, which is superior to the AIC.\n\nThe BIC, or Bayesian information criterion, attempts to approximate the posterior probability that each model is best.\n\nThe BIC can be useful only when in small sample settings.\n\nThe pseudo R-squared is a goodness-of-fit measure of how much of the deviance is “explained” by the model.\n\n\n# the AIC\nAIC &lt;- 2 * ( length( model$coefficients ) -\n             loglikelihood( as.numeric(train$atRisk), pred) )\nAIC\n\n[1] 2490.992\n\nmodel$aic\n\n[1] 2490.992\n\n# the pseudo R-squared\npseudo_r2 &lt;- 1 - (resid.dev / null.dev)\n\n\nRegression preserves the probabilities:\n\n\ntrain$pred &lt;- predict(model, newdata=train, type = \"response\")\ntest$pred &lt;- predict(model, newdata=test, type=\"response\")\n\nsum(train$atRisk == TRUE)\n\n[1] 273\n\nsum(train$pred)\n\n[1] 273\n\npremature &lt;- subset(train, GESTREC3 == \"&lt; 37 weeks\")\nsum(premature$atRisk == TRUE)\n\n[1] 112\n\nsum(premature$pred)\n\n[1] 112\n\n\n\n\n\nAverage Marginal Effects\n\nm &lt;- margins(model)\name_result &lt;- summary(m)\name_result\n\n                   factor     AME     SE       z      p   lower   upper\n                  CIG_REC  0.0064 0.0043  1.5000 0.1336 -0.0020  0.0148\n DPLURALtriplet or higher  0.0484 0.0290  1.6677 0.0954 -0.0085  0.1052\n              DPLURALtwin  0.0064 0.0056  1.1480 0.2510 -0.0045  0.0173\n       GESTREC3&lt; 37 weeks  0.0450 0.0062  7.2235 0.0000  0.0328  0.0571\n                     PWGT  0.0001 0.0000  2.5096 0.0121  0.0000  0.0001\n               ULD_BREECH  0.0181 0.0056  3.2510 0.0012  0.0072  0.0290\n                 ULD_MECO  0.0211 0.0082  2.5718 0.0101  0.0050  0.0371\n               ULD_PRECIP  0.0038 0.0077  0.4946 0.6209 -0.0113  0.0189\n                  UPREVIS -0.0012 0.0003 -4.0631 0.0000 -0.0017 -0.0006\n               URF_CHYPER  0.0131 0.0115  1.1437 0.2528 -0.0094  0.0356\n                 URF_DIAB -0.0055 0.0040 -1.3887 0.1649 -0.0133  0.0023\n                URF_ECLAM  0.0114 0.0220  0.5196 0.6033 -0.0317  0.0545\n               URF_PHYPER  0.0031 0.0052  0.6066 0.5441 -0.0070  0.0133\n\nggplot(data = ame_result) +\n  geom_point( aes(factor, AME) ) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper), \n                width = .5) +\n  geom_hline(yintercept = 0) +\n  coord_flip()"
  },
  {
    "objectID": "mba/mba-ch1-logit.html#references",
    "href": "mba/mba-ch1-logit.html#references",
    "title": "Logistic Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount."
  },
  {
    "objectID": "mba/mba-ch7-tree-based-methods.html#classification-and-regression-tree-cart",
    "href": "mba/mba-ch7-tree-based-methods.html#classification-and-regression-tree-cart",
    "title": "Tree-based Methods",
    "section": "Classification And Regression Tree (CART)",
    "text": "Classification And Regression Tree (CART)\n\n\n\n\n\n\n\n\n\n\nTree-logic uses a series of steps to come to a conclusion.\nThe trick is to have mini-decisions combine for good choices.\nEach decision is a node, and the final prediction is a leaf node.\n\n\nDecision trees are useful for both classification and regression.\n\nDecision trees can take any type of data, numerical or categorical.\nDecision trees make fewer assumptions about the relationship between x and y.\n\nE.g., linear model assumes the linear relationship between x and y.\nDecision trees naturally express certain kinds of interactions among the input variables: those of the form “IF x is true AND y is true, THEN….”\n\n\n\n\nClassification trees have class probabilities at the leaves.\n\nProbability I’ll be in heavy rain is 0.9 (so take an umbrella).\n\nRegression trees have a mean response at the leaves.\n\nThe expected amount of rain is 2 inches (so take an umbrella).\n\nCART: Classification and Regression Trees.\n\n\n\nWe need a way to estimate the sequence of decisions.\n\nHow many are they?\nWhat is the order?\n\nCART grows the tree through a sequence of splits:\n\nGiven any set (node) of data, we can find the optimal split (the error minimizing split) and divide into two child sets.\nWe then look at each child set, and again find the optimal split to divide it into two homogeneous subsets.\nThe children become parents, and we look again for the optimal split on their new children (the grandchildren!).\n\nWe stop splitting and growing when the size of the leaf nodes hits some minimum threshold (e.g., say no less than 10 observations per leaf).\n\n\nObjective at each split: find the best variable to partition the data into one of two regions, \\(R_1\\) & \\(R_2\\), to minimize the error between the actual response, \\(y_i\\), and the node’s predicted constant, \\(c_i\\)\n\nFor regression we minimize the sum of squared errors (SSE):\n\n\\[\nS S E=\\sum_{i \\in R_{1}}\\left(y_{i}-c_{1}\\right)^{2}+\\sum_{i \\in R_{2}}\\left(y_{i}-c_{2}\\right)^{2}\n\\]\n\nFor classification trees we minimize the node’s impurity the Gini index\n\nwhere \\(p_k\\) is the proportion of observations in the node belonging to class \\(k\\) out of \\(K\\) total classes\nwant to minimize \\(Gini\\): small values indicate a node has primarily one class (is more pure)\nGini impurity measures the degree of a particular variable being wrongly classified when it is randomly chosen.\n\n\n\\[\nGini = 1 - \\sum_k^K p_k^2\n\\]\n\n\nNBC Show Data\n\nThe dataset (nbc and demog) is from NBC’s TV pilots:\n\nGross Ratings Points (GRP): estimated total viewership, which measures broadcast marketability.\nProjected Engagement (PE): a more suitable measure of audience.\nAfter watching a show, viewer is quizzed on order and detail.\nThis measures their engagement with the show (and ads!).\n\n\n\nlibrary(tidyverse)\nnbc &lt;- read_csv('https://bcdanl.github.io/data/nbc_show.csv')\nnbc$Genre &lt;- as.factor(nbc$Genre)\n\n\n\n\n\n  \n\n\n\n\nskim(nbc)\n\n\nData summary\n\n\nName\nnbc\n\n\nNumber of rows\n40\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nShow\n0\n1\n4\n34\n0\n40\n0\n\n\nNetwork\n0\n1\n2\n5\n0\n14\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGenre\n0\n1\nFALSE\n3\nDra: 19, Rea: 17, Sit: 4\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPE\n0\n1\n72.68\n12.03\n30.0\n64.58\n77.06\n80.16\n89.29\n▁▁▃▃▇\n\n\nGRP\n0\n1\n823.90\n683.42\n7.5\n301.45\n647.90\n1200.45\n2773.80\n▇▅▂▂▁\n\n\nDuration\n0\n1\n50.25\n14.23\n30.0\n30.00\n60.00\n60.00\n60.00\n▃▁▁▁▇\n\n\n\n\n\n\nggplot(nbc) +\n  geom_point(aes(x = GRP, y = PE, color = Genre),\n             alpha = .75)\n\n\n\n\n\nConsider a classification tree to predict Genre from demographics.\n\nOutput from tree shows a series of decision nodes and the proportion in each Genre at these nodes, down to the leaves.\n\n\n\ndemog &lt;- read_csv(\n  'https://bcdanl.github.io/data/nbc_demog.csv'\n)\n\n\n\n\n\n  \n\n\n\n\nskim(demog)\n\n\nData summary\n\n\nName\ndemog\n\n\nNumber of rows\n40\n\n\nNumber of columns\n57\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n56\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nShow\n0\n1\n4\n34\n0\n40\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTERRITORY.EAST.CENTRAL\n0\n1\n13.51\n2.78\n6.16\n11.73\n14.05\n15.36\n19.98\n▁▃▆▇▁\n\n\nTERRITORY.NORTHEAST\n0\n1\n20.35\n3.17\n14.01\n18.04\n20.31\n22.49\n27.80\n▂▇▇▅▂\n\n\nTERRITORY.PACIFIC\n0\n1\n17.42\n3.67\n9.47\n15.24\n17.72\n19.68\n28.79\n▃▃▇▁▁\n\n\nTERRITORY.SOUTHEAST\n0\n1\n21.56\n3.68\n16.23\n19.21\n21.32\n22.92\n37.11\n▇▇▂▁▁\n\n\nTERRITORY.SOUTHWEST\n0\n1\n11.83\n2.29\n7.68\n10.47\n11.58\n13.45\n16.24\n▃▇▇▅▅\n\n\nTERRITORY.WEST.CENTRAL\n0\n1\n15.33\n3.15\n9.64\n12.99\n15.01\n16.93\n22.69\n▂▇▇▂▂\n\n\nCOUNTY.SIZE.A\n0\n1\n39.27\n6.01\n28.36\n35.14\n39.55\n42.75\n51.55\n▃▇▇▃▃\n\n\nCOUNTY.SIZE.B\n0\n1\n32.14\n3.29\n22.30\n30.65\n32.04\n33.69\n38.60\n▁▁▇▆▂\n\n\nCOUNTY.SIZE.C\n0\n1\n14.67\n2.73\n8.24\n13.00\n14.66\n16.37\n20.55\n▂▅▇▇▂\n\n\nCOUNTY.SIZE.D\n0\n1\n13.93\n3.80\n4.47\n11.92\n14.11\n16.60\n24.95\n▂▆▇▅▁\n\n\nWIRED.CABLE.W.PAY\n0\n1\n34.63\n6.01\n24.79\n30.34\n34.74\n36.97\n48.91\n▅▅▇▂▂\n\n\nWIRED.CABLE.W.O.PAY\n0\n1\n29.58\n5.33\n21.95\n25.73\n28.45\n34.24\n43.60\n▇▇▅▂▂\n\n\nDBS.OWNER\n0\n1\n23.60\n4.85\n10.03\n20.47\n24.85\n26.50\n31.83\n▁▂▃▇▃\n\n\nBROADCAST.ONLY\n0\n1\n12.72\n11.42\n0.00\n0.00\n14.04\n20.96\n37.96\n▇▃▆▂▁\n\n\nVIDEO.GAME.OWNER\n0\n1\n57.20\n4.91\n49.02\n53.61\n56.03\n60.48\n67.87\n▂▇▃▂▂\n\n\nDVD.OWNER\n0\n1\n92.77\n2.15\n87.44\n91.44\n92.73\n94.23\n98.46\n▁▇▇▇▁\n\n\nVCR.OWNER\n0\n1\n84.37\n3.42\n74.14\n82.66\n85.40\n86.55\n90.40\n▁▁▃▇▂\n\n\nX1.TV.SET\n0\n1\n11.53\n3.28\n1.54\n9.90\n11.11\n13.95\n19.30\n▁▂▇▂▂\n\n\nX2.TV.SETS\n0\n1\n27.03\n2.71\n21.72\n25.29\n27.18\n28.49\n34.05\n▂▆▇▃▁\n\n\nX3.TV.SETS\n0\n1\n28.37\n3.43\n21.72\n26.53\n28.14\n29.67\n39.85\n▂▇▅▁▁\n\n\nX4..TV.SETS\n0\n1\n33.08\n3.41\n27.48\n31.14\n32.64\n34.70\n43.72\n▃▇▃▁▁\n\n\nX1.PERSON\n0\n1\n10.92\n3.54\n1.78\n9.16\n10.90\n12.99\n21.98\n▁▃▇▂▁\n\n\nX2.PERSONS\n0\n1\n25.15\n2.69\n19.54\n23.32\n25.43\n26.58\n30.58\n▃▃▇▇▃\n\n\nX3.PERSONS\n0\n1\n22.84\n2.00\n18.85\n21.61\n22.61\n23.62\n28.41\n▂▇▆▂▂\n\n\nX4..PERSONS\n0\n1\n41.09\n4.72\n28.41\n39.12\n40.59\n42.96\n56.75\n▁▇▇▂▁\n\n\nHOH..25\n0\n1\n6.52\n3.78\n0.83\n4.34\n6.00\n7.96\n20.36\n▆▇▂▁▁\n\n\nHOH.25.34\n0\n1\n25.07\n4.75\n16.17\n21.21\n26.06\n28.69\n32.47\n▅▃▅▇▅\n\n\nHOH.35.44\n0\n1\n33.70\n4.58\n19.13\n31.34\n33.93\n36.08\n45.02\n▁▁▇▆▁\n\n\nHOH.45.54\n0\n1\n26.01\n4.17\n14.76\n23.03\n25.59\n28.89\n33.18\n▁▂▇▅▅\n\n\nHOH.55.64\n0\n1\n5.70\n1.60\n2.49\n4.40\n6.00\n7.00\n8.37\n▃▆▅▇▆\n\n\nHOH.65.\n0\n1\n3.01\n1.09\n0.55\n2.46\n3.03\n3.85\n4.92\n▁▅▇▅▃\n\n\nX1.3.YRS.COLLEGE\n0\n1\n32.90\n2.70\n26.18\n31.67\n32.63\n34.39\n41.94\n▂▇▇▁▁\n\n\nX4..YRS.COLLEGE\n0\n1\n29.72\n8.94\n11.16\n24.29\n29.54\n35.60\n47.39\n▂▃▇▃▂\n\n\nX4.YRS.H.S.\n0\n1\n29.50\n5.90\n16.22\n25.11\n29.68\n33.23\n42.57\n▂▅▇▅▂\n\n\nWHITE.COLLAR\n0\n1\n50.85\n7.38\n30.92\n47.97\n51.62\n56.47\n60.99\n▁▃▅▇▇\n\n\nBLUE.COLLAR\n0\n1\n30.20\n4.93\n22.05\n27.63\n29.59\n32.95\n46.99\n▃▇▃▁▁\n\n\nNOT.IN.LABOR.FORCE\n0\n1\n18.95\n4.66\n8.86\n16.08\n17.57\n22.09\n31.64\n▁▇▅▁▁\n\n\nBLACK\n0\n1\n14.00\n5.66\n2.56\n10.74\n12.90\n16.48\n35.34\n▂▇▆▁▁\n\n\nWHITE\n0\n1\n78.54\n7.03\n53.96\n75.71\n78.21\n83.15\n91.53\n▁▁▅▇▃\n\n\nOTHER\n0\n1\n7.46\n2.29\n4.06\n5.98\n6.65\n8.55\n12.58\n▅▇▃▂▂\n\n\nANY.CHILDREN.2.5\n0\n1\n18.17\n3.13\n12.16\n15.89\n17.98\n20.06\n25.21\n▃▅▇▃▂\n\n\nANY.CHILDREN.6.11\n0\n1\n22.75\n3.98\n13.64\n21.03\n22.70\n24.60\n36.49\n▂▇▇▁▁\n\n\nANY.CHILDREN.12.17\n0\n1\n24.37\n4.53\n17.80\n22.30\n24.11\n25.57\n40.40\n▃▇▁▁▁\n\n\nANY.CATS\n0\n1\n33.98\n3.76\n26.32\n31.31\n33.98\n36.49\n41.59\n▂▇▅▇▂\n\n\nANY.DOGS\n0\n1\n48.46\n4.47\n36.77\n46.77\n48.53\n50.67\n62.94\n▁▅▇▂▁\n\n\nMALE.HOH\n0\n1\n48.87\n5.05\n36.26\n46.02\n48.83\n51.89\n56.89\n▂▂▇▇▆\n\n\nFEMALE.HOH\n0\n1\n51.14\n5.06\n43.10\n48.12\n51.13\n53.99\n63.97\n▆▇▇▂▂\n\n\nINCOME.30.74K.\n0\n1\n41.74\n3.55\n31.04\n40.23\n41.48\n43.64\n54.32\n▁▃▇▁▁\n\n\nINCOME.75K.\n0\n1\n37.22\n8.82\n20.11\n29.95\n37.78\n43.89\n56.87\n▃▅▅▇▁\n\n\nHISPANIC.ORIGIN\n0\n1\n8.18\n3.34\n3.85\n5.60\n7.85\n9.75\n19.56\n▇▆▂▁▁\n\n\nNON.HISPANIC.ORIGIN\n0\n1\n91.82\n3.35\n80.34\n90.21\n92.12\n94.42\n96.15\n▁▁▂▆▇\n\n\nHOME.IS.OWNED\n0\n1\n70.27\n7.56\n52.59\n67.42\n73.01\n75.54\n80.88\n▃▂▃▇▇\n\n\nHOME.IS.RENTED\n0\n1\n29.72\n7.55\n19.12\n24.46\n26.99\n32.58\n47.39\n▇▇▃▂▃\n\n\nPC.NON.OWNER\n0\n1\n15.53\n5.99\n5.80\n11.80\n14.00\n19.82\n28.37\n▃▇▃▃▂\n\n\nPC.OWNER.WITH.INTERNET.ACCESS\n0\n1\n75.70\n7.65\n59.35\n70.90\n77.11\n80.58\n88.17\n▃▃▆▇▅\n\n\nPC.OWNER.WITHOUT.INTERNET.ACCESS\n0\n1\n8.77\n2.57\n2.03\n7.29\n8.35\n9.78\n15.20\n▁▃▇▂▂\n\n\n\n\n\n\n# install.packages(c(\"tree\",\"randomForest\",\"ranger\", \"rpart\", \"vip\", \"pdp\", \"caret\"))\nlibrary(tree)\ngenretree &lt;- tree(nbc$Genre ~ ., \n                  data = demog[,-1], \n                  mincut = 1)\nnbc$genrepred &lt;- predict(genretree, \n                         newdata = demog[,-1], \n                         type = \"class\")\n\n\n# tree plot (dendrogram)\nplot(genretree, col=8, lwd=2)\ntext(genretree, label=\"yprob\")\n\n\n\n\nConsider predicting engagement from ratings and genre.\n\nLeaf predictions are expected engagement.\n\n\n# mincut=1 allows for leaves containing a single show,\n# with expected engagement that single show's PE.\nnbctree &lt;- tree(PE ~ Genre + GRP, data=nbc[,-1], mincut=1)\nnbc$PEpred &lt;- predict(nbctree, newdata=nbc[,-1])\n\n## tree plot (dendrogram)\nplot(nbctree, col=8, lwd=2)\ntext(nbctree)\n\n\n\nggplot(nbc) +\n  geom_point(aes(x = GRP, y = PE, color = Genre) ) +\n  geom_line(aes(x = GRP, y = PEpred, color = Genre) )\n\n\n\n\n\nPE increases with GRP, but in jumps!"
  },
  {
    "objectID": "mba/mba-ch7-tree-based-methods.html#cv-tree",
    "href": "mba/mba-ch7-tree-based-methods.html#cv-tree",
    "title": "Tree-based Methods",
    "section": "CV Tree",
    "text": "CV Tree\n\nThe biggest challenge with CART models is avoiding overfit.\n\nFor CART, the usual solution is to rely on cross validation (CV).\nThe way to cross-validate the fully fitted tree is to prune it by removing split rules from the bottom up:\n\nAt each step, remove the split that contributes least to deviance reduction.\n\nThis is a reverse to CART’s growth process.\n\nPruning yields candidate tree.\n\nEach prune step produces a candidate tree model, and we can compare their out-of-sample prediction performance through CV.\n\n\n\n\nBoston Housing Data\n\nThe MASS package includes the Boston data.frame, which has 506 observations and 14 variables.\n\ncrim: per capita crime rate by town.\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\nindus: proportion of non-retail business acres per town.\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nnox: nitrogen oxides concentration (parts per 10 million).\nrm: average number of rooms per dwelling.\nage: proportion of owner-occupied units built prior to 1940.\ndis: weighted mean of distances to five Boston employment centres.\nrad: index of accessibility to radial highways.\ntax: full-value property-tax rate per $10,000.\nptratio: pupil-teacher ratio by town.\nblack: \\(1000(Bk - 0.63)^2\\) where \\(Bk\\) is the proportion of blacks by town.\nlstat: lower status of the population (percent).\nmedv: median value of owner-occupied homes in $1000s.\n\nFor more details about the data set, try ?Boston.\nThe goal is to predict housing values.\n\n\nlibrary(MASS)\n?Boston\nBoston &lt;- MASS::Boston\n\n\n\n\n\n  \n\n\n\n\nskim(MASS::Boston)\n\n\nData summary\n\n\nName\nMASS::Boston\n\n\nNumber of rows\n506\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncrim\n0\n1\n3.61\n8.60\n0.01\n0.08\n0.26\n3.68\n88.98\n▇▁▁▁▁\n\n\nzn\n0\n1\n11.36\n23.32\n0.00\n0.00\n0.00\n12.50\n100.00\n▇▁▁▁▁\n\n\nindus\n0\n1\n11.14\n6.86\n0.46\n5.19\n9.69\n18.10\n27.74\n▇▆▁▇▁\n\n\nchas\n0\n1\n0.07\n0.25\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nnox\n0\n1\n0.55\n0.12\n0.38\n0.45\n0.54\n0.62\n0.87\n▇▇▆▅▁\n\n\nrm\n0\n1\n6.28\n0.70\n3.56\n5.89\n6.21\n6.62\n8.78\n▁▂▇▂▁\n\n\nage\n0\n1\n68.57\n28.15\n2.90\n45.02\n77.50\n94.07\n100.00\n▂▂▂▃▇\n\n\ndis\n0\n1\n3.80\n2.11\n1.13\n2.10\n3.21\n5.19\n12.13\n▇▅▂▁▁\n\n\nrad\n0\n1\n9.55\n8.71\n1.00\n4.00\n5.00\n24.00\n24.00\n▇▂▁▁▃\n\n\ntax\n0\n1\n408.24\n168.54\n187.00\n279.00\n330.00\n666.00\n711.00\n▇▇▃▁▇\n\n\nptratio\n0\n1\n18.46\n2.16\n12.60\n17.40\n19.05\n20.20\n22.00\n▁▃▅▅▇\n\n\nblack\n0\n1\n356.67\n91.29\n0.32\n375.38\n391.44\n396.22\n396.90\n▁▁▁▁▇\n\n\nlstat\n0\n1\n12.65\n7.14\n1.73\n6.95\n11.36\n16.96\n37.97\n▇▇▅▂▁\n\n\nmedv\n0\n1\n22.53\n9.20\n5.00\n17.02\n21.20\n25.00\n50.00\n▂▇▅▁▁\n\n\n\n\n\n\nSpliting training and testing data\n\n\nset.seed(42120532)\nindex &lt;- sample(nrow(Boston),nrow(Boston)*0.80)\nBoston.train &lt;- Boston[index,]\nBoston.test &lt;- Boston[-index,]\n\n\nA bit of data visualization\n\n\nBoston_vis &lt;- Boston %&gt;%\n  gather(-medv, key = \"var\", value = \"value\") %&gt;% \n  filter(var != \"chas\")\n\nggplot(Boston_vis, aes(x = value, y = medv)) +\n  geom_point(alpha = .33) +\n  geom_smooth() +\n  facet_wrap(~ var, scales = \"free\") \n\n\n\nggplot(Boston_vis, aes(y = value)) +\n  geom_boxplot(outlier.color = \"red\", outlier.shape = 1) +\n  facet_wrap(~ var, scales = \"free\") \n\n\n\nggplot(Boston_vis, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ var, scales = \"free\") \n\n\n\n\n\nrpart()\n\nRuns 10-fold CV tree to tune \\(\\alpha\\) (CP) for pruning.\nSelects the number of terminal nodes via 1-SE rule.\n\n\n\nlibrary(rpart)\nboston_tree &lt;- rpart(medv ~ .,\n                     data = Boston.train, method  = \"anova\")\nboston_tree\n\nn= 404 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 404 34343.3600 22.47178  \n   2) lstat&gt;=9.725 237  5424.9600 17.30084  \n     4) lstat&gt;=18.825 67  1000.6320 12.28358  \n       8) nox&gt;=0.603 51   417.5804 10.88039 *\n       9) nox&lt; 0.603 16   162.5594 16.75625 *\n     5) lstat&lt; 18.825 170  2073.0290 19.27824  \n      10) lstat&gt;=14.395 73   656.7096 17.12877 *\n      11) lstat&lt; 14.395 97   825.2184 20.89588 *\n   3) lstat&lt; 9.725 167 13588.0500 29.81018  \n     6) rm&lt; 7.4525 143  5781.9690 27.19790  \n      12) dis&gt;=1.95265 135  3329.1170 26.35185  \n        24) rm&lt; 6.722 88   927.6799 23.74886 *\n        25) rm&gt;=6.722 47   688.8094 31.22553 *\n      13) dis&lt; 1.95265 8   725.5350 41.47500 *\n     7) rm&gt;=7.4525 24  1015.9250 45.37500 *\n\n\n\nprintcp displays cp table for Fitted rpart() object:\n\n\nprintcp(boston_tree)\n\n\nRegression tree:\nrpart(formula = medv ~ ., data = Boston.train, method = \"anova\")\n\nVariables actually used in tree construction:\n[1] dis   lstat nox   rm   \n\nRoot node error: 34343/404 = 85.008\n\nn= 404 \n\n        CP nsplit rel error  xerror     xstd\n1 0.446385      0   1.00000 1.00634 0.095395\n2 0.197714      1   0.55362 0.65150 0.067953\n3 0.068464      2   0.35590 0.39758 0.050182\n4 0.050296      3   0.28744 0.32195 0.047053\n5 0.049868      4   0.23714 0.32917 0.048158\n6 0.017212      5   0.18727 0.29925 0.045811\n7 0.012244      6   0.17006 0.27112 0.046339\n8 0.010000      7   0.15782 0.27249 0.046513\n\n\n\nrpart.plot() plots the estimated tree structure from an rpart() object.\n\nWith method = \"anova\" (a continuous outcome variable), each node shows:\n\nthe predicted value;\nthe percentage of observations in the node.\n\n\n\n\nlibrary(rpart.plot)\nrpart.plot(boston_tree)\n\n\n\n\n\nWith method = \"class\" (a binary outcome variable), each node will show:\n\nthe predicted class;\nthe predicted probability;\nthe percentage of observations in the node.\n\n\n\n\nplotcp() gives a visual representation of the cross-validation results in an rpart() object.\n\nThe size of a decision tree is the number of leaf nodes (non-terminal nodes) in the tree.\n\n\n\nplotcp(boston_tree)\n\n\n\n\n\nWhat about the full tree? (cp = 0)\n\nThe control parameter in rpart() allows for controlling the rpart fit. (see rpart.fit)\n\ncp: complexity parameter. the minimum improvement in the model needed at each node.\n\nThe higher the cp, the smaller the size of tree.\n\nxval: number of cross-validations\n\n\n\n\nfull_boston_tree &lt;- rpart(formula = medv ~ .,\n                       data = Boston.train, method  = \"anova\", \n                       control = list(cp = 0, xval = 10))\n\n\nrpart.plot(full_boston_tree)\n\n\n\n\n\nCompare the full tree with the pruned tree.\n\nWhich variable is not included in the pruned tree?\n\n\n\nplotcp(full_boston_tree)\n\n\n\n\n\nWe can train the CV trees with the caret package as well:\n\n\nlibrary(caret)\ncaret_boston_tree &lt;- train(medv ~ .,\n                        data = Boston.train, method = \"rpart\",\n                        trControl = trainControl(method = \"cv\", number = 10),\n                        tuneLength = 20)\nggplot(caret_boston_tree)\n\n\n\n\n\nrpart.plot(caret_boston_tree$finalModel)"
  },
  {
    "objectID": "mba/mba-ch7-tree-based-methods.html#random-forest",
    "href": "mba/mba-ch7-tree-based-methods.html#random-forest",
    "title": "Tree-based Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nWhy should we try other tree models?\n\nCART automatically learns non-linear response functions and will discover interactions between variables.\n\nUnfortunately, it is tough to avoid overfit with CART.\nHigh variance, i.e. split a dataset in half and grow tress in each half, the result will be very different\n\n\nCART generally results in higher test set error rates.\nReal structure of the tree is not easily chosen via cross-validation (CV).\n\nOne way to mitigate the shortcomings of CART is bootstrap aggregation, or bagging.\n\n\n\n\nBagging Algorithm\n\n\n\n\n\n\n\n\n\n\nBootstrap is random sampling with replacement.\nAggregation is combining the results from many trees together, each constructed with a different bootstrapped sample of the data.\n\n\n\n\n\n\n\n\n\n\n\nReal structure that persists across datasets shows up in the average.\nA bagged ensemble of trees is also less likely to overfit the data.\nTo generate a prediction for a new point:\n\nRegression: take the average across the trees\nClassification: take the majority vote across the trees\n\nassuming each tree predicts a single class (could use probabilities instead…)\n\n\nBagging improves prediction accuracy via wisdom of the crowds but at the expense of interpretability.\n\nEasy to read one tree, but how do we read 500 trees?\nHowever, we can still use the measures of variable importance and partial dependence to summarize our models.\n\n\n\n\n\nRandom Forest Algorithm\n\nRandom forests are an extension of bagging.\n\nAt each split, the algorithm limits the variables considered to a random subset \\(m_{try}\\) of the given \\(p\\) number of variables.\nIt introduce \\(m_{try}\\) as a tuning parameter: typically use \\(p/3\\) for regression or \\(\\sqrt{p}\\) for classification.\n\n\n\n\n\n\n\n\n\n\n\n\nSplit-variable randomization adds more randomness to make each tree more independent of each other.\nThe final ensemble of trees is bagged to make the random forest predictions.\n\n\n\nSince the trees are constructed via bootstrapped data (samples with replacements), each sample is likely to have duplicate observations.\nOut-of-bag (OOB), original observations not contained in a single bootstrap sample, can be used to make out-of-sample predictive performance of the model.\n\n\n\nIntuition behind the Random Forest Algorithm\n\nThe reason Random Forest algorithm considers a random subset of features at each split in the decision tree is to increase the diversity among the individual trees in the forest.\n\nThis is a method to make the model more robust and prevent overfitting.\n\nIf all the variables were considered at each split, each decision tree in the forest would look more similar, as they would likely use the same (or very similar) variables for splitting, especially the first few splits which typically have the most impact on the structure of the tree.\n\nThis is because some features might be so informative that they would always be chosen for splits early in the tree construction process if all features were considered.\nThis would make the trees in the forest correlated, which would reduce the power of the ensemble.\n\nBy considering only a random subset of the variables at each split, we increase the chance that less dominant variables are considered, leading to a more diverse set of trees.\n\nThis diversity is key to the power of the Random Forest algorithm, as it allows for a more robust prediction that is less likely to overfit the training data.\nIt also helps to reduce the variance of the predictions, as the errors of the individual trees are likely to cancel each other out when averaged (for regression) or voted on (for classification).\n\n\n\n\nranger package is a popular & fast implementation (see randomForest for the original).\n\nLet’s consider the estimation with randomForest first.\n\n\n\nlibrary(randomForest)\nbag.boston &lt;- randomForest(medv ~ ., data = Boston.train, \n                           mtry=13, ntree = 50,\n                           importance =TRUE)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston.train, mtry = 13,      ntree = 50, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 50\nNo. of variables tried at each split: 13\n\n          Mean of squared residuals: 10.94723\n                    % Var explained: 87.12\n\nplot(bag.boston)\n\n\n\n\n\nNow Let’s consider the estimation with ranger.\n\n\nlibrary(ranger)\nbag.boston_ranger &lt;- ranger(medv ~ ., data = Boston.train, \n                            mtry = 13, num.trees = 50,\n                            importance = \"impurity\")\nbag.boston_ranger\n\nRanger result\n\nCall:\n ranger(medv ~ ., data = Boston.train, mtry = 13, num.trees = 50,      importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  50 \nSample size:                      404 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       10.58043 \nR squared (OOB):                  0.8758446 \n\n\n\n\nCV Tree vs. Random Forest\n\nWe can compare the performance of the CV CART and the RF via MSE.\nMSE from CV Tree\n\n\n# prediction\nboston.train.pred.CART &lt;- predict(boston_tree, Boston.train)\nboston.test.pred.CART &lt;- predict(boston_tree, Boston.test)\n\n# MSE\nmean((Boston.test$medv - boston.test.pred.CART)^2)\n\n[1] 22.94158\n\nmean((Boston.train$medv - boston.train.pred.CART)^2)\n\n[1] 13.41588\n\n\n\nMSE from Random Forest\n\n\n# prediction\nboston.train.pred.RF &lt;- predict(bag.boston_ranger, Boston.train)$predictions\nboston.test.pred.RF &lt;- predict(bag.boston_ranger, Boston.test)$predictions\n\n# MSE\nmean((Boston.test$medv - boston.test.pred.RF)^2)\n\n[1] 11.24414\n\nmean((Boston.train$medv - boston.train.pred.RF)^2)\n\n[1] 1.987043\n\n\n\n\n\n\nVariable Importance in the Tree-based Models\n\nVariable importance is measured based on reduction in SSE.\n\nMean Decrease Accuracy (% increase in MSE): This shows how much our model accuracy decreases if we leave out that variable.\nMean Decrease Gini (Increase in Node Purity) : This is a measure of variable importance based on the Gini impurity index used for the calculating the splits in trees.\n\n\n\n\nOut-of-bag samples for datum x1\n\n\n\n\n\n\n\n\nCalculating variable importance of variable v1\n\n\n\n\n\n\n\n\nSince we set importance not to equal to \"none\" when using rpart, caret, and ranger, we can evaluate variable importance using the left-out sample.\n\n\nvip(caret_boston_tree, geom = \"point\")\n\n\n\n\n\nvip(full_boston_tree, geom = \"point\")\n\n\n\n\n\nvip(boston_tree, geom = \"point\")\n\n\n\n\n\nvip(bag.boston_ranger, geom = \"point\")\n\n\n\n\n\n\nWe can also summarize the relationship between a predictor and the predicted outcome using a partial dependence plot\n\n\nlibrary(pdp)\n# predictor, lstat\npartial(bag.boston_ranger, pred.var = \"lstat\") %&gt;% autoplot()\n\n\n\n\n\n# predictor, rm\npartial(bag.boston_ranger, pred.var = \"rm\") %&gt;% autoplot()\n\n\n\n\n\n# predictor, rad\npartial(bag.boston_ranger, pred.var = \"rad\") %&gt;% autoplot()"
  },
  {
    "objectID": "mba/mba-ch7-tree-based-methods.html#references",
    "href": "mba/mba-ch7-tree-based-methods.html#references",
    "title": "Tree-based Methods",
    "section": "References",
    "text": "References\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount.\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HONR 393-14: Capstone in Data Analytics, Fall 2025",
    "section": "",
    "text": "Welcome! 👋\n\\(-\\) Explore, Learn, and Grow with Data Analytics! 🌟"
  },
  {
    "objectID": "index.html#what-the-report-should-contain",
    "href": "index.html#what-the-report-should-contain",
    "title": "HONR 393-14: Capstone in Data Analytics, Fall 2025",
    "section": "📝 What the “Report” Should Contain",
    "text": "📝 What the “Report” Should Contain\nEach report is meant to serve as a structured progress documentation that balances technical work and personal learning. You should:\n\nSummarize progress\n\nWhat was done since the last meeting? (e.g., data collection, preparation, exploration, modeling)\nWhat challenges were encountered?\n\nEvaluate methods\n\nAre chosen tools/approaches working well?\nWhat adjustments are needed?\n\nDocument insights\n\nWhat new findings emerged from the data?\nWhat was learned about the research question so far?\n\nPlan ahead\n\nWhat are the next concrete steps before the next meeting?\nIdentify support/resources needed.\n\nPersonal takeaways\n\nBriefly reflect on skills gained (coding, data wrangling, communication, teamwork).\nHow do these skills connect to the overall capstone or professional goals?"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "HONR 393-14: Capstone in Data Analytics, Fall 2025",
    "section": "✍️ Assignments",
    "text": "✍️ Assignments\n\nResearch Kickoff Report\n\nResearch ideas, data sources, and first challenges\n\nProgress & Insights Report\n\nWhat’s been done and early analyses\n\nResearch Synthesis Report\n\nTakeaways, limitations, lessons learned, and next steps for the final proposal\n\n\n\n📑 Components of the Final Proposal\n\nTitle & Abstract\n\nConcise project title.\n150–250 word abstract summarizing research question, data, methods, and expected contribution.\n\nIntroduction & Motivation\n\nBackground on the problem/topic.\nWhy it matters (business, economics, or policy relevance).\nClear statement of research question(s).\n\nLiterature & Context (short)\n\nWhat have others done in this area?\nHow does this project extend or differ?\n\nData\n\nDescription of the dataset(s): source, size, variables, time period.\nHow data were collected (APIs, scraping, public repositories, etc.).\nAny limitations (missing data, measurement error).\n\nMethods / Analytical Approach\n\nWhat techniques will be used (e.g., regression, clustering, NLP, time-series)?\nWhy are these methods appropriate?\nAny expected preprocessing steps (cleaning, transformations).\n\nPreliminary Results / Exploration\n\nEarly exploratory data analysis (EDA).\nTables, summary statistics, or sample visualizations.\nEvidence the student/team has actually interacted with the dataset.\n\nTimeline & Work Plan\n\nBreakdown of remaining work (analysis, visualization, writing).\nRoles and responsibilities if a team project.\n\nExpected Contributions\n\nWhat new insights do you expect to generate?\nWho benefits from this research (scholars, firms, policymakers, the public)?\n\nReferences\n\nCited literature, data sources, and tools (APA or other standard).\n\nAppendices (Optional)\n\nExtra figures, code snippets, or data dictionaries."
  }
]